ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
========
Switching to PyTorch attention since your Xformers is broken.
========

Unsloth: Xformers does not work in RTX 50X, Blackwell GPUs as of yet. Please build from source via
```
pip install ninja
pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
```

ü¶• Unsloth Zoo will now patch everything to make training faster!
wandb: Currently logged in as: knowrohit (projectsolomon) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run t48e8396
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /workspace/nyu-dl/wandb/run-20251102_021646-t48e8396
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run config5_r1_alpha2_lr2e4_maxsteps60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/projectsolomon/math-verification-multiconfig
wandb: üöÄ View run at https://wandb.ai/projectsolomon/math-verification-multiconfig/runs/t48e8396
wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

================================================================================
Training Configuration: config5_r1_alpha2_lr2e4_maxsteps60
GPU: 0 | LoRA r: 1 | LoRA alpha: 2 | LR: 0.0002
Batch size: 2 | Grad accum: 4
Max seq length: 1024 | Samples: 10000
Training duration: 60 steps
================================================================================

Loading model and tokenizer...
==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.477 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading dataset...
Training samples: 10000
Validation samples: 500
Formatting training dataset...
Setting up LoRA...
Unsloth 2025.10.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Configuring trainer...

================================================================================
Starting training: config5_r1_alpha2_lr2e4_maxsteps60
================================================================================

==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 2,621,440 of 8,032,882,688 (0.03% trained)
  0%|          | 0/60 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
  2%|‚ñè         | 1/60 [00:05<05:35,  5.69s/it]  3%|‚ñé         | 2/60 [00:10<05:12,  5.39s/it]  5%|‚ñå         | 3/60 [00:15<04:34,  4.82s/it]  7%|‚ñã         | 4/60 [00:18<04:08,  4.43s/it]  8%|‚ñä         | 5/60 [00:22<03:43,  4.06s/it] 10%|‚ñà         | 6/60 [00:25<03:24,  3.79s/it] 12%|‚ñà‚ñè        | 7/60 [00:29<03:26,  3.90s/it] 13%|‚ñà‚ñé        | 8/60 [00:32<03:10,  3.66s/it] 15%|‚ñà‚ñå        | 9/60 [00:36<03:06,  3.67s/it] 17%|‚ñà‚ñã        | 10/60 [00:40<03:07,  3.74s/it]                                               {'loss': 1.4839, 'grad_norm': 0.5885986685752869, 'learning_rate': 0.00018545454545454545, 'epoch': 0.01}
 17%|‚ñà‚ñã        | 10/60 [00:40<03:07,  3.74s/it] 18%|‚ñà‚ñä        | 11/60 [00:43<02:56,  3.60s/it] 20%|‚ñà‚ñà        | 12/60 [00:47<03:00,  3.77s/it] 22%|‚ñà‚ñà‚ñè       | 13/60 [00:51<02:53,  3.68s/it] 23%|‚ñà‚ñà‚ñé       | 14/60 [00:55<03:03,  3.99s/it] 25%|‚ñà‚ñà‚ñå       | 15/60 [00:59<02:58,  3.96s/it] 27%|‚ñà‚ñà‚ñã       | 16/60 [01:04<02:58,  4.05s/it] 28%|‚ñà‚ñà‚ñä       | 17/60 [01:07<02:42,  3.77s/it] 30%|‚ñà‚ñà‚ñà       | 18/60 [01:10<02:37,  3.76s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [01:14<02:34,  3.76s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [01:18<02:32,  3.81s/it]                                               {'loss': 1.1068, 'grad_norm': 0.5683570504188538, 'learning_rate': 0.0001490909090909091, 'epoch': 0.02}
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [01:18<02:32,  3.81s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [01:22<02:32,  3.90s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [01:27<02:38,  4.16s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [01:30<02:20,  3.81s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [01:35<02:24,  4.03s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [01:38<02:18,  3.94s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [01:42<02:14,  3.94s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [01:47<02:13,  4.06s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [01:51<02:13,  4.18s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [01:55<02:08,  4.14s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [02:00<02:07,  4.27s/it]                                               {'loss': 0.8263, 'grad_norm': 0.42210444808006287, 'learning_rate': 0.00011272727272727272, 'epoch': 0.02}
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [02:00<02:07,  4.27s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [02:04<02:01,  4.21s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [02:07<01:52,  4.01s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [02:11<01:47,  3.98s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [02:16<01:49,  4.22s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [02:22<01:56,  4.65s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [02:25<01:43,  4.30s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [02:30<01:40,  4.38s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [02:34<01:38,  4.50s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [02:38<01:25,  4.09s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [02:41<01:18,  3.92s/it]                                               {'loss': 0.8369, 'grad_norm': 0.40950921177864075, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.03}
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [02:41<01:18,  3.92s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [02:45<01:13,  3.87s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [02:48<01:08,  3.79s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [02:52<01:04,  3.78s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [02:56<00:59,  3.70s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [02:59<00:54,  3.61s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [03:04<00:55,  3.95s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [03:07<00:48,  3.72s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [03:11<00:46,  3.90s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [03:16<00:43,  3.99s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [03:19<00:38,  3.86s/it]                                               {'loss': 0.7852, 'grad_norm': 0.39615148305892944, 'learning_rate': 4e-05, 'epoch': 0.04}
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [03:19<00:38,  3.86s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [03:22<00:32,  3.66s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [03:26<00:29,  3.71s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [03:30<00:27,  3.87s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [03:36<00:26,  4.44s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [03:40<00:21,  4.21s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [03:43<00:15,  3.86s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [03:47<00:11,  3.94s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [03:51<00:07,  3.82s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [03:55<00:03,  3.89s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [03:58<00:00,  3.84s/it]                                               {'loss': 0.7949, 'grad_norm': 0.4651467502117157, 'learning_rate': 3.636363636363636e-06, 'epoch': 0.05}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [03:58<00:00,  3.84s/it]                                               {'train_runtime': 241.2174, 'train_samples_per_second': 1.99, 'train_steps_per_second': 0.249, 'train_loss': 0.9723345041275024, 'epoch': 0.05}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [04:01<00:00,  3.84s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [04:01<00:00,  4.02s/it]

================================================================================
Training completed: config5_r1_alpha2_lr2e4_maxsteps60
================================================================================

Saving model...
Done! Model saved to ./models/config5_r1_alpha2_lr2e4_maxsteps60
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: uploading history steps 5-6, summary, console lines 35-35
wandb: uploading console lines 38-51
wandb: 
wandb: Run history:
wandb:         train/epoch ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà
wandb:   train/global_step ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà
wandb:     train/grad_norm ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÑ
wandb: train/learning_rate ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb:          train/loss ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:               total_flos 7572381280223232.0
wandb:              train/epoch 0.048
wandb:        train/global_step 60
wandb:          train/grad_norm 0.46515
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.7949
wandb:               train_loss 0.97233
wandb:            train_runtime 241.2174
wandb: train_samples_per_second 1.99
wandb:   train_steps_per_second 0.249
wandb: 
wandb: üöÄ View run config5_r1_alpha2_lr2e4_maxsteps60 at: https://wandb.ai/projectsolomon/math-verification-multiconfig/runs/t48e8396
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/projectsolomon/math-verification-multiconfig
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251102_021646-t48e8396/logs
