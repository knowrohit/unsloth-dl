{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008,
      "grad_norm": 0.7118391394615173,
      "learning_rate": 0.0001997863818424566,
      "loss": 1.3737,
      "step": 10
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.5292805433273315,
      "learning_rate": 0.00019925233644859814,
      "loss": 0.8544,
      "step": 20
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.4531402885913849,
      "learning_rate": 0.00019871829105473968,
      "loss": 0.7364,
      "step": 30
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.38496822118759155,
      "learning_rate": 0.0001981842456608812,
      "loss": 0.7777,
      "step": 40
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.33739954233169556,
      "learning_rate": 0.0001976502002670227,
      "loss": 0.7374,
      "step": 50
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.3664347529411316,
      "learning_rate": 0.00019711615487316423,
      "loss": 0.7485,
      "step": 60
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.34502580761909485,
      "learning_rate": 0.00019658210947930574,
      "loss": 0.7394,
      "step": 70
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.28246670961380005,
      "learning_rate": 0.00019604806408544728,
      "loss": 0.6841,
      "step": 80
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.3492569029331207,
      "learning_rate": 0.0001955140186915888,
      "loss": 0.6711,
      "step": 90
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.37163180112838745,
      "learning_rate": 0.00019497997329773033,
      "loss": 0.71,
      "step": 100
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.34964844584465027,
      "learning_rate": 0.00019444592790387183,
      "loss": 0.7503,
      "step": 110
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.39200663566589355,
      "learning_rate": 0.00019391188251001334,
      "loss": 0.6859,
      "step": 120
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.3590065538883209,
      "learning_rate": 0.00019337783711615488,
      "loss": 0.7309,
      "step": 130
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.39595115184783936,
      "learning_rate": 0.00019284379172229642,
      "loss": 0.6773,
      "step": 140
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3447250723838806,
      "learning_rate": 0.00019230974632843793,
      "loss": 0.6791,
      "step": 150
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5987209677696228,
      "learning_rate": 0.00019177570093457943,
      "loss": 0.7087,
      "step": 160
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.35280054807662964,
      "learning_rate": 0.00019124165554072097,
      "loss": 0.7172,
      "step": 170
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.32454773783683777,
      "learning_rate": 0.00019070761014686248,
      "loss": 0.7833,
      "step": 180
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.28768447041511536,
      "learning_rate": 0.00019017356475300402,
      "loss": 0.7456,
      "step": 190
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.3350850045681,
      "learning_rate": 0.00018963951935914555,
      "loss": 0.6929,
      "step": 200
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.43154099583625793,
      "learning_rate": 0.00018910547396528706,
      "loss": 0.7191,
      "step": 210
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.32593297958374023,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.7187,
      "step": 220
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.3385140597820282,
      "learning_rate": 0.00018803738317757008,
      "loss": 0.6782,
      "step": 230
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.29273122549057007,
      "learning_rate": 0.00018750333778371164,
      "loss": 0.702,
      "step": 240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.3258448839187622,
      "learning_rate": 0.00018696929238985315,
      "loss": 0.7257,
      "step": 250
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.3529664874076843,
      "learning_rate": 0.00018643524699599466,
      "loss": 0.6726,
      "step": 260
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.326656699180603,
      "learning_rate": 0.0001859012016021362,
      "loss": 0.6919,
      "step": 270
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.32359984517097473,
      "learning_rate": 0.0001853671562082777,
      "loss": 0.6986,
      "step": 280
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.36231333017349243,
      "learning_rate": 0.00018483311081441924,
      "loss": 0.6751,
      "step": 290
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.31444910168647766,
      "learning_rate": 0.00018429906542056075,
      "loss": 0.7213,
      "step": 300
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.4028303027153015,
      "learning_rate": 0.0001837650200267023,
      "loss": 0.7079,
      "step": 310
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.3735502064228058,
      "learning_rate": 0.0001832309746328438,
      "loss": 0.6711,
      "step": 320
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.3765380382537842,
      "learning_rate": 0.0001826969292389853,
      "loss": 0.7206,
      "step": 330
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.3724675476551056,
      "learning_rate": 0.00018216288384512684,
      "loss": 0.707,
      "step": 340
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2861682176589966,
      "learning_rate": 0.00018162883845126838,
      "loss": 0.6462,
      "step": 350
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.35083872079849243,
      "learning_rate": 0.0001810947930574099,
      "loss": 0.6404,
      "step": 360
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.3344983160495758,
      "learning_rate": 0.0001805607476635514,
      "loss": 0.6646,
      "step": 370
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.3001914620399475,
      "learning_rate": 0.00018002670226969293,
      "loss": 0.721,
      "step": 380
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.3419458270072937,
      "learning_rate": 0.00017949265687583444,
      "loss": 0.6516,
      "step": 390
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.32889726758003235,
      "learning_rate": 0.00017895861148197598,
      "loss": 0.7138,
      "step": 400
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.4319423735141754,
      "learning_rate": 0.00017842456608811751,
      "loss": 0.6586,
      "step": 410
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.3599138855934143,
      "learning_rate": 0.00017789052069425902,
      "loss": 0.6668,
      "step": 420
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2560847997665405,
      "learning_rate": 0.00017735647530040053,
      "loss": 0.7265,
      "step": 430
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.3512897193431854,
      "learning_rate": 0.00017682242990654207,
      "loss": 0.7185,
      "step": 440
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.3038640022277832,
      "learning_rate": 0.00017628838451268358,
      "loss": 0.6506,
      "step": 450
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.41097283363342285,
      "learning_rate": 0.00017575433911882511,
      "loss": 0.6832,
      "step": 460
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.40175485610961914,
      "learning_rate": 0.00017522029372496662,
      "loss": 0.7084,
      "step": 470
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.3040468394756317,
      "learning_rate": 0.00017468624833110816,
      "loss": 0.667,
      "step": 480
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.2903330624103546,
      "learning_rate": 0.00017415220293724967,
      "loss": 0.6251,
      "step": 490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2956523895263672,
      "learning_rate": 0.00017361815754339118,
      "loss": 0.6639,
      "step": 500
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.33146294951438904,
      "learning_rate": 0.00017308411214953274,
      "loss": 0.678,
      "step": 510
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.29687127470970154,
      "learning_rate": 0.00017255006675567425,
      "loss": 0.6973,
      "step": 520
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.3325817883014679,
      "learning_rate": 0.00017201602136181576,
      "loss": 0.6835,
      "step": 530
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.3989435136318207,
      "learning_rate": 0.00017148197596795727,
      "loss": 0.7298,
      "step": 540
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.2771569490432739,
      "learning_rate": 0.0001709479305740988,
      "loss": 0.646,
      "step": 550
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.4623313546180725,
      "learning_rate": 0.00017041388518024034,
      "loss": 0.7299,
      "step": 560
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.33643192052841187,
      "learning_rate": 0.00016987983978638185,
      "loss": 0.6597,
      "step": 570
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.3094630837440491,
      "learning_rate": 0.0001693457943925234,
      "loss": 0.6637,
      "step": 580
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.3384397029876709,
      "learning_rate": 0.0001688117489986649,
      "loss": 0.6817,
      "step": 590
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.45021164417266846,
      "learning_rate": 0.0001682777036048064,
      "loss": 0.6326,
      "step": 600
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.42056697607040405,
      "learning_rate": 0.00016774365821094794,
      "loss": 0.7193,
      "step": 610
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.3514828681945801,
      "learning_rate": 0.00016720961281708948,
      "loss": 0.692,
      "step": 620
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.39226803183555603,
      "learning_rate": 0.000166675567423231,
      "loss": 0.6818,
      "step": 630
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.39112675189971924,
      "learning_rate": 0.0001661415220293725,
      "loss": 0.6403,
      "step": 640
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.3100140392780304,
      "learning_rate": 0.00016560747663551403,
      "loss": 0.6722,
      "step": 650
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.3738604485988617,
      "learning_rate": 0.00016507343124165554,
      "loss": 0.6856,
      "step": 660
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.3883800804615021,
      "learning_rate": 0.00016453938584779708,
      "loss": 0.6783,
      "step": 670
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.3345675468444824,
      "learning_rate": 0.00016400534045393859,
      "loss": 0.6788,
      "step": 680
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.2699117660522461,
      "learning_rate": 0.00016347129506008012,
      "loss": 0.6459,
      "step": 690
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.2950584888458252,
      "learning_rate": 0.00016293724966622163,
      "loss": 0.6307,
      "step": 700
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.3724830448627472,
      "learning_rate": 0.00016240320427236314,
      "loss": 0.6946,
      "step": 710
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.33156266808509827,
      "learning_rate": 0.00016186915887850468,
      "loss": 0.7006,
      "step": 720
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.30392324924468994,
      "learning_rate": 0.0001613351134846462,
      "loss": 0.6462,
      "step": 730
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.33573251962661743,
      "learning_rate": 0.00016080106809078772,
      "loss": 0.6711,
      "step": 740
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.3324897587299347,
      "learning_rate": 0.00016026702269692923,
      "loss": 0.6513,
      "step": 750
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.3405260741710663,
      "learning_rate": 0.00015973297730307077,
      "loss": 0.6956,
      "step": 760
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.30288517475128174,
      "learning_rate": 0.00015919893190921228,
      "loss": 0.6236,
      "step": 770
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.3502545952796936,
      "learning_rate": 0.0001586648865153538,
      "loss": 0.6662,
      "step": 780
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.3805295526981354,
      "learning_rate": 0.00015813084112149535,
      "loss": 0.6476,
      "step": 790
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.38597536087036133,
      "learning_rate": 0.00015759679572763686,
      "loss": 0.6904,
      "step": 800
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.436593234539032,
      "learning_rate": 0.00015706275033377837,
      "loss": 0.7141,
      "step": 810
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.5099055767059326,
      "learning_rate": 0.0001565287049399199,
      "loss": 0.6596,
      "step": 820
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.3190918266773224,
      "learning_rate": 0.00015599465954606144,
      "loss": 0.6988,
      "step": 830
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.33283731341362,
      "learning_rate": 0.00015546061415220295,
      "loss": 0.6584,
      "step": 840
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.31947532296180725,
      "learning_rate": 0.00015492656875834446,
      "loss": 0.6451,
      "step": 850
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.3269800841808319,
      "learning_rate": 0.000154392523364486,
      "loss": 0.7472,
      "step": 860
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.43052002787590027,
      "learning_rate": 0.0001538584779706275,
      "loss": 0.6492,
      "step": 870
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.4450041949748993,
      "learning_rate": 0.00015332443257676904,
      "loss": 0.6543,
      "step": 880
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.3800365924835205,
      "learning_rate": 0.00015279038718291055,
      "loss": 0.6702,
      "step": 890
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.3681153953075409,
      "learning_rate": 0.00015225634178905209,
      "loss": 0.6487,
      "step": 900
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.31400156021118164,
      "learning_rate": 0.0001517222963951936,
      "loss": 0.656,
      "step": 910
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.3220032751560211,
      "learning_rate": 0.0001511882510013351,
      "loss": 0.6508,
      "step": 920
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.35699018836021423,
      "learning_rate": 0.00015065420560747664,
      "loss": 0.6558,
      "step": 930
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.38744401931762695,
      "learning_rate": 0.00015012016021361818,
      "loss": 0.6984,
      "step": 940
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.484500914812088,
      "learning_rate": 0.00014958611481975968,
      "loss": 0.69,
      "step": 950
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.42699411511421204,
      "learning_rate": 0.00014905206942590122,
      "loss": 0.6331,
      "step": 960
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.3738514184951782,
      "learning_rate": 0.00014851802403204273,
      "loss": 0.703,
      "step": 970
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.3286191523075104,
      "learning_rate": 0.00014798397863818424,
      "loss": 0.6628,
      "step": 980
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.4038204848766327,
      "learning_rate": 0.00014744993324432578,
      "loss": 0.6655,
      "step": 990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.38862401247024536,
      "learning_rate": 0.0001469158878504673,
      "loss": 0.6217,
      "step": 1000
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.37090060114860535,
      "learning_rate": 0.00014638184245660882,
      "loss": 0.6568,
      "step": 1010
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.410186767578125,
      "learning_rate": 0.00014584779706275033,
      "loss": 0.6552,
      "step": 1020
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.3208751678466797,
      "learning_rate": 0.00014531375166889187,
      "loss": 0.7489,
      "step": 1030
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.3489096760749817,
      "learning_rate": 0.00014477970627503338,
      "loss": 0.6296,
      "step": 1040
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.4018864929676056,
      "learning_rate": 0.0001442456608811749,
      "loss": 0.6477,
      "step": 1050
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.36595460772514343,
      "learning_rate": 0.00014371161548731642,
      "loss": 0.6767,
      "step": 1060
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.44845911860466003,
      "learning_rate": 0.00014317757009345796,
      "loss": 0.6856,
      "step": 1070
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.3974864184856415,
      "learning_rate": 0.00014264352469959947,
      "loss": 0.6596,
      "step": 1080
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.4186108112335205,
      "learning_rate": 0.000142109479305741,
      "loss": 0.6682,
      "step": 1090
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.3586856424808502,
      "learning_rate": 0.00014157543391188254,
      "loss": 0.6556,
      "step": 1100
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.39187607169151306,
      "learning_rate": 0.00014104138851802405,
      "loss": 0.6449,
      "step": 1110
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.4795498549938202,
      "learning_rate": 0.00014050734312416556,
      "loss": 0.6978,
      "step": 1120
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.30038517713546753,
      "learning_rate": 0.00013997329773030707,
      "loss": 0.6236,
      "step": 1130
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.4575892388820648,
      "learning_rate": 0.0001394392523364486,
      "loss": 0.6092,
      "step": 1140
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.365311861038208,
      "learning_rate": 0.00013890520694259014,
      "loss": 0.6783,
      "step": 1150
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.3561428487300873,
      "learning_rate": 0.00013837116154873165,
      "loss": 0.6417,
      "step": 1160
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.5481484532356262,
      "learning_rate": 0.00013783711615487318,
      "loss": 0.7128,
      "step": 1170
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.33331435918807983,
      "learning_rate": 0.0001373030707610147,
      "loss": 0.6558,
      "step": 1180
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.34868040680885315,
      "learning_rate": 0.0001367690253671562,
      "loss": 0.6591,
      "step": 1190
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3966173529624939,
      "learning_rate": 0.00013623497997329774,
      "loss": 0.6985,
      "step": 1200
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.34826424717903137,
      "learning_rate": 0.00013570093457943927,
      "loss": 0.6363,
      "step": 1210
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.32022711634635925,
      "learning_rate": 0.00013516688918558078,
      "loss": 0.6404,
      "step": 1220
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.3619239926338196,
      "learning_rate": 0.0001346328437917223,
      "loss": 0.6451,
      "step": 1230
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.45864689350128174,
      "learning_rate": 0.00013409879839786383,
      "loss": 0.6959,
      "step": 1240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.36603787541389465,
      "learning_rate": 0.00013356475300400534,
      "loss": 0.6747,
      "step": 1250
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.4741908013820648,
      "learning_rate": 0.00013303070761014687,
      "loss": 0.5677,
      "step": 1260
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.46139493584632874,
      "learning_rate": 0.00013249666221628838,
      "loss": 0.5928,
      "step": 1270
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.3996303081512451,
      "learning_rate": 0.00013196261682242992,
      "loss": 0.5635,
      "step": 1280
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.501745879650116,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.5692,
      "step": 1290
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.4582846462726593,
      "learning_rate": 0.00013089452603471294,
      "loss": 0.5933,
      "step": 1300
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.4108402132987976,
      "learning_rate": 0.0001303604806408545,
      "loss": 0.6008,
      "step": 1310
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.3582436442375183,
      "learning_rate": 0.000129826435246996,
      "loss": 0.5901,
      "step": 1320
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.4956722855567932,
      "learning_rate": 0.00012929238985313752,
      "loss": 0.5958,
      "step": 1330
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.42934805154800415,
      "learning_rate": 0.00012875834445927903,
      "loss": 0.5963,
      "step": 1340
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.5341302752494812,
      "learning_rate": 0.00012822429906542056,
      "loss": 0.5899,
      "step": 1350
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.5878846049308777,
      "learning_rate": 0.0001276902536715621,
      "loss": 0.5924,
      "step": 1360
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.5028613209724426,
      "learning_rate": 0.0001271562082777036,
      "loss": 0.602,
      "step": 1370
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.4432745575904846,
      "learning_rate": 0.00012662216288384515,
      "loss": 0.6083,
      "step": 1380
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.4463781714439392,
      "learning_rate": 0.00012608811748998666,
      "loss": 0.5677,
      "step": 1390
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.41471943259239197,
      "learning_rate": 0.00012555407209612816,
      "loss": 0.6012,
      "step": 1400
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.5046402812004089,
      "learning_rate": 0.0001250200267022697,
      "loss": 0.6039,
      "step": 1410
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.49641773104667664,
      "learning_rate": 0.00012448598130841124,
      "loss": 0.621,
      "step": 1420
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.36467888951301575,
      "learning_rate": 0.00012395193591455275,
      "loss": 0.5617,
      "step": 1430
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.4644210636615753,
      "learning_rate": 0.00012341789052069426,
      "loss": 0.6023,
      "step": 1440
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.45744264125823975,
      "learning_rate": 0.0001228838451268358,
      "loss": 0.6022,
      "step": 1450
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.4633462429046631,
      "learning_rate": 0.0001223497997329773,
      "loss": 0.5367,
      "step": 1460
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.42654603719711304,
      "learning_rate": 0.00012181575433911882,
      "loss": 0.5897,
      "step": 1470
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.4768052101135254,
      "learning_rate": 0.00012128170894526036,
      "loss": 0.6042,
      "step": 1480
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.5616681575775146,
      "learning_rate": 0.00012074766355140188,
      "loss": 0.5987,
      "step": 1490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.6046043038368225,
      "learning_rate": 0.00012021361815754339,
      "loss": 0.5433,
      "step": 1500
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.6128513813018799,
      "learning_rate": 0.00011967957276368491,
      "loss": 0.5871,
      "step": 1510
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.5433838367462158,
      "learning_rate": 0.00011914552736982645,
      "loss": 0.6181,
      "step": 1520
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.5177456736564636,
      "learning_rate": 0.00011861148197596796,
      "loss": 0.5885,
      "step": 1530
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.5645192861557007,
      "learning_rate": 0.00011807743658210948,
      "loss": 0.5979,
      "step": 1540
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.4283658564090729,
      "learning_rate": 0.00011754339118825102,
      "loss": 0.5972,
      "step": 1550
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.4951283037662506,
      "learning_rate": 0.00011700934579439253,
      "loss": 0.6022,
      "step": 1560
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.4985368847846985,
      "learning_rate": 0.00011647530040053405,
      "loss": 0.5668,
      "step": 1570
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.7224979400634766,
      "learning_rate": 0.00011594125500667556,
      "loss": 0.596,
      "step": 1580
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.4480522871017456,
      "learning_rate": 0.0001154072096128171,
      "loss": 0.5969,
      "step": 1590
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.45931950211524963,
      "learning_rate": 0.00011487316421895862,
      "loss": 0.5817,
      "step": 1600
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.5252848863601685,
      "learning_rate": 0.00011433911882510013,
      "loss": 0.5886,
      "step": 1610
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.559486448764801,
      "learning_rate": 0.00011380507343124168,
      "loss": 0.6866,
      "step": 1620
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.5494579076766968,
      "learning_rate": 0.00011327102803738319,
      "loss": 0.6036,
      "step": 1630
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.4879581928253174,
      "learning_rate": 0.00011273698264352471,
      "loss": 0.596,
      "step": 1640
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.5396652221679688,
      "learning_rate": 0.00011220293724966622,
      "loss": 0.5614,
      "step": 1650
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.6373549699783325,
      "learning_rate": 0.00011166889185580775,
      "loss": 0.5638,
      "step": 1660
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.5145823955535889,
      "learning_rate": 0.00011113484646194928,
      "loss": 0.5519,
      "step": 1670
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.6030011773109436,
      "learning_rate": 0.00011060080106809079,
      "loss": 0.5881,
      "step": 1680
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.44490107893943787,
      "learning_rate": 0.00011006675567423232,
      "loss": 0.5919,
      "step": 1690
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.43996620178222656,
      "learning_rate": 0.00010953271028037384,
      "loss": 0.5811,
      "step": 1700
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.47266602516174316,
      "learning_rate": 0.00010899866488651535,
      "loss": 0.5893,
      "step": 1710
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.44962459802627563,
      "learning_rate": 0.00010846461949265688,
      "loss": 0.5714,
      "step": 1720
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.468990296125412,
      "learning_rate": 0.00010793057409879841,
      "loss": 0.5941,
      "step": 1730
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.5613850355148315,
      "learning_rate": 0.00010739652870493992,
      "loss": 0.5389,
      "step": 1740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.47238558530807495,
      "learning_rate": 0.00010686248331108144,
      "loss": 0.623,
      "step": 1750
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.5026029944419861,
      "learning_rate": 0.00010632843791722298,
      "loss": 0.5634,
      "step": 1760
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.5833238959312439,
      "learning_rate": 0.00010579439252336449,
      "loss": 0.5875,
      "step": 1770
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.6338862180709839,
      "learning_rate": 0.00010526034712950601,
      "loss": 0.5879,
      "step": 1780
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.578133225440979,
      "learning_rate": 0.00010472630173564752,
      "loss": 0.632,
      "step": 1790
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.4468608796596527,
      "learning_rate": 0.00010419225634178906,
      "loss": 0.5945,
      "step": 1800
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.5115406513214111,
      "learning_rate": 0.00010365821094793058,
      "loss": 0.5059,
      "step": 1810
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.5695720314979553,
      "learning_rate": 0.00010312416555407209,
      "loss": 0.5971,
      "step": 1820
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.4810065031051636,
      "learning_rate": 0.00010259012016021363,
      "loss": 0.6096,
      "step": 1830
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.5020376443862915,
      "learning_rate": 0.00010205607476635515,
      "loss": 0.5525,
      "step": 1840
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.5188254714012146,
      "learning_rate": 0.00010152202937249666,
      "loss": 0.6342,
      "step": 1850
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.6068457961082458,
      "learning_rate": 0.00010098798397863818,
      "loss": 0.5779,
      "step": 1860
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.46107903122901917,
      "learning_rate": 0.00010045393858477972,
      "loss": 0.5753,
      "step": 1870
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.5188695788383484,
      "learning_rate": 9.991989319092123e-05,
      "loss": 0.5593,
      "step": 1880
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.520192563533783,
      "learning_rate": 9.938584779706276e-05,
      "loss": 0.6078,
      "step": 1890
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.4331505298614502,
      "learning_rate": 9.885180240320427e-05,
      "loss": 0.5633,
      "step": 1900
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.6058899164199829,
      "learning_rate": 9.831775700934581e-05,
      "loss": 0.5477,
      "step": 1910
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.5653498768806458,
      "learning_rate": 9.778371161548732e-05,
      "loss": 0.5742,
      "step": 1920
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.4469345211982727,
      "learning_rate": 9.724966622162884e-05,
      "loss": 0.5938,
      "step": 1930
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.5182225108146667,
      "learning_rate": 9.671562082777038e-05,
      "loss": 0.5682,
      "step": 1940
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.538972795009613,
      "learning_rate": 9.618157543391188e-05,
      "loss": 0.5537,
      "step": 1950
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.4596411883831024,
      "learning_rate": 9.564753004005341e-05,
      "loss": 0.5399,
      "step": 1960
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.5631588697433472,
      "learning_rate": 9.511348464619493e-05,
      "loss": 0.6018,
      "step": 1970
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.5049766302108765,
      "learning_rate": 9.457943925233645e-05,
      "loss": 0.581,
      "step": 1980
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.7947441935539246,
      "learning_rate": 9.404539385847798e-05,
      "loss": 0.5793,
      "step": 1990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.6259042024612427,
      "learning_rate": 9.35113484646195e-05,
      "loss": 0.5619,
      "step": 2000
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.4810004234313965,
      "learning_rate": 9.297730307076102e-05,
      "loss": 0.5768,
      "step": 2010
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.5997658967971802,
      "learning_rate": 9.244325767690254e-05,
      "loss": 0.5671,
      "step": 2020
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.7011457085609436,
      "learning_rate": 9.190921228304407e-05,
      "loss": 0.5648,
      "step": 2030
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.529826819896698,
      "learning_rate": 9.137516688918557e-05,
      "loss": 0.6014,
      "step": 2040
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.6212543249130249,
      "learning_rate": 9.084112149532711e-05,
      "loss": 0.5607,
      "step": 2050
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.4775844216346741,
      "learning_rate": 9.030707610146862e-05,
      "loss": 0.5616,
      "step": 2060
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.5320963859558105,
      "learning_rate": 8.977303070761016e-05,
      "loss": 0.5552,
      "step": 2070
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.4202576279640198,
      "learning_rate": 8.923898531375168e-05,
      "loss": 0.5746,
      "step": 2080
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.6126434206962585,
      "learning_rate": 8.870493991989319e-05,
      "loss": 0.532,
      "step": 2090
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.5872557759284973,
      "learning_rate": 8.817089452603472e-05,
      "loss": 0.5362,
      "step": 2100
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.5318357944488525,
      "learning_rate": 8.763684913217623e-05,
      "loss": 0.5974,
      "step": 2110
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.4584988057613373,
      "learning_rate": 8.710280373831776e-05,
      "loss": 0.5532,
      "step": 2120
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.5992835164070129,
      "learning_rate": 8.656875834445929e-05,
      "loss": 0.5932,
      "step": 2130
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.4969613254070282,
      "learning_rate": 8.60347129506008e-05,
      "loss": 0.5713,
      "step": 2140
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.5549280643463135,
      "learning_rate": 8.550066755674232e-05,
      "loss": 0.56,
      "step": 2150
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.5509365797042847,
      "learning_rate": 8.496662216288385e-05,
      "loss": 0.6032,
      "step": 2160
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.5131396055221558,
      "learning_rate": 8.443257676902537e-05,
      "loss": 0.5694,
      "step": 2170
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.5351954698562622,
      "learning_rate": 8.389853137516689e-05,
      "loss": 0.5358,
      "step": 2180
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.47459304332733154,
      "learning_rate": 8.336448598130842e-05,
      "loss": 0.5755,
      "step": 2190
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.5648199319839478,
      "learning_rate": 8.283044058744994e-05,
      "loss": 0.5222,
      "step": 2200
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.6275856494903564,
      "learning_rate": 8.229639519359146e-05,
      "loss": 0.5393,
      "step": 2210
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.650908887386322,
      "learning_rate": 8.176234979973298e-05,
      "loss": 0.5697,
      "step": 2220
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.5558348894119263,
      "learning_rate": 8.12283044058745e-05,
      "loss": 0.5507,
      "step": 2230
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.4450554847717285,
      "learning_rate": 8.069425901201603e-05,
      "loss": 0.5493,
      "step": 2240
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5987076759338379,
      "learning_rate": 8.016021361815754e-05,
      "loss": 0.5632,
      "step": 2250
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.5660477876663208,
      "learning_rate": 7.962616822429907e-05,
      "loss": 0.5944,
      "step": 2260
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.5947235822677612,
      "learning_rate": 7.90921228304406e-05,
      "loss": 0.6185,
      "step": 2270
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.48974329233169556,
      "learning_rate": 7.85580774365821e-05,
      "loss": 0.5403,
      "step": 2280
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.5770321488380432,
      "learning_rate": 7.802403204272364e-05,
      "loss": 0.5607,
      "step": 2290
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.6267406344413757,
      "learning_rate": 7.748998664886515e-05,
      "loss": 0.5562,
      "step": 2300
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.7129994034767151,
      "learning_rate": 7.695594125500669e-05,
      "loss": 0.5545,
      "step": 2310
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.5983807444572449,
      "learning_rate": 7.64218958611482e-05,
      "loss": 0.596,
      "step": 2320
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.5677525401115417,
      "learning_rate": 7.588785046728972e-05,
      "loss": 0.5575,
      "step": 2330
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.6698184609413147,
      "learning_rate": 7.535380507343126e-05,
      "loss": 0.5708,
      "step": 2340
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.6217545866966248,
      "learning_rate": 7.481975967957276e-05,
      "loss": 0.5741,
      "step": 2350
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.5428702235221863,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.5641,
      "step": 2360
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.5983683466911316,
      "learning_rate": 7.375166889185581e-05,
      "loss": 0.5674,
      "step": 2370
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.5897618532180786,
      "learning_rate": 7.321762349799733e-05,
      "loss": 0.5902,
      "step": 2380
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.5569369792938232,
      "learning_rate": 7.268357810413885e-05,
      "loss": 0.5524,
      "step": 2390
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.5610100030899048,
      "learning_rate": 7.214953271028038e-05,
      "loss": 0.6441,
      "step": 2400
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.5985649824142456,
      "learning_rate": 7.16154873164219e-05,
      "loss": 0.5765,
      "step": 2410
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.5546858310699463,
      "learning_rate": 7.108144192256342e-05,
      "loss": 0.5994,
      "step": 2420
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.5157601833343506,
      "learning_rate": 7.054739652870495e-05,
      "loss": 0.5786,
      "step": 2430
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.5795364379882812,
      "learning_rate": 7.001335113484645e-05,
      "loss": 0.5433,
      "step": 2440
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.564928412437439,
      "learning_rate": 6.947930574098799e-05,
      "loss": 0.6162,
      "step": 2450
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.7421690225601196,
      "learning_rate": 6.894526034712951e-05,
      "loss": 0.5266,
      "step": 2460
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.4986265301704407,
      "learning_rate": 6.841121495327104e-05,
      "loss": 0.546,
      "step": 2470
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.6171109080314636,
      "learning_rate": 6.787716955941256e-05,
      "loss": 0.5741,
      "step": 2480
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.5900023579597473,
      "learning_rate": 6.734312416555407e-05,
      "loss": 0.5379,
      "step": 2490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.46911394596099854,
      "learning_rate": 6.68090787716956e-05,
      "loss": 0.5925,
      "step": 2500
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.6217893958091736,
      "learning_rate": 6.627503337783711e-05,
      "loss": 0.5062,
      "step": 2510
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.5822582244873047,
      "learning_rate": 6.574098798397864e-05,
      "loss": 0.4606,
      "step": 2520
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.6909774541854858,
      "learning_rate": 6.520694259012017e-05,
      "loss": 0.4871,
      "step": 2530
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.6169356107711792,
      "learning_rate": 6.467289719626168e-05,
      "loss": 0.4494,
      "step": 2540
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.5042609572410583,
      "learning_rate": 6.41388518024032e-05,
      "loss": 0.4695,
      "step": 2550
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.7632601857185364,
      "learning_rate": 6.360480640854473e-05,
      "loss": 0.4891,
      "step": 2560
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.6681459546089172,
      "learning_rate": 6.307076101468625e-05,
      "loss": 0.4606,
      "step": 2570
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.5634186863899231,
      "learning_rate": 6.253671562082777e-05,
      "loss": 0.4312,
      "step": 2580
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.6047733426094055,
      "learning_rate": 6.20026702269693e-05,
      "loss": 0.4886,
      "step": 2590
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.7206020355224609,
      "learning_rate": 6.146862483311082e-05,
      "loss": 0.4679,
      "step": 2600
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.6578206419944763,
      "learning_rate": 6.093457943925234e-05,
      "loss": 0.4845,
      "step": 2610
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.6423745155334473,
      "learning_rate": 6.040053404539386e-05,
      "loss": 0.5678,
      "step": 2620
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.7232933044433594,
      "learning_rate": 5.986648865153538e-05,
      "loss": 0.4473,
      "step": 2630
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.6605302095413208,
      "learning_rate": 5.933244325767691e-05,
      "loss": 0.4652,
      "step": 2640
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.5967273116111755,
      "learning_rate": 5.8798397863818424e-05,
      "loss": 0.4418,
      "step": 2650
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.6792703866958618,
      "learning_rate": 5.826435246995995e-05,
      "loss": 0.4567,
      "step": 2660
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.6754416823387146,
      "learning_rate": 5.7730307076101476e-05,
      "loss": 0.4658,
      "step": 2670
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.6717085838317871,
      "learning_rate": 5.719626168224299e-05,
      "loss": 0.4983,
      "step": 2680
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.6763043403625488,
      "learning_rate": 5.6662216288384515e-05,
      "loss": 0.5102,
      "step": 2690
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.6783806681632996,
      "learning_rate": 5.612817089452603e-05,
      "loss": 0.4837,
      "step": 2700
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.6781232953071594,
      "learning_rate": 5.559412550066756e-05,
      "loss": 0.4736,
      "step": 2710
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.567658543586731,
      "learning_rate": 5.506008010680909e-05,
      "loss": 0.4903,
      "step": 2720
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.728402853012085,
      "learning_rate": 5.4526034712950606e-05,
      "loss": 0.4802,
      "step": 2730
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.6968241333961487,
      "learning_rate": 5.399198931909213e-05,
      "loss": 0.4759,
      "step": 2740
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.7044956684112549,
      "learning_rate": 5.3457943925233644e-05,
      "loss": 0.4641,
      "step": 2750
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.6562018394470215,
      "learning_rate": 5.2923898531375174e-05,
      "loss": 0.4593,
      "step": 2760
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.553147554397583,
      "learning_rate": 5.238985313751669e-05,
      "loss": 0.4567,
      "step": 2770
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.6725966334342957,
      "learning_rate": 5.185580774365821e-05,
      "loss": 0.444,
      "step": 2780
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.9845023155212402,
      "learning_rate": 5.132176234979974e-05,
      "loss": 0.5183,
      "step": 2790
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.867740273475647,
      "learning_rate": 5.078771695594126e-05,
      "loss": 0.4866,
      "step": 2800
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.6543918251991272,
      "learning_rate": 5.025367156208278e-05,
      "loss": 0.4658,
      "step": 2810
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.8589489459991455,
      "learning_rate": 4.97196261682243e-05,
      "loss": 0.4889,
      "step": 2820
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.7759212851524353,
      "learning_rate": 4.9185580774365825e-05,
      "loss": 0.4987,
      "step": 2830
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.6959983110427856,
      "learning_rate": 4.865153538050735e-05,
      "loss": 0.4867,
      "step": 2840
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.5590757727622986,
      "learning_rate": 4.8117489986648864e-05,
      "loss": 0.4613,
      "step": 2850
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.5900548100471497,
      "learning_rate": 4.758344459279039e-05,
      "loss": 0.4491,
      "step": 2860
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.7124440670013428,
      "learning_rate": 4.704939919893191e-05,
      "loss": 0.462,
      "step": 2870
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.638396143913269,
      "learning_rate": 4.651535380507344e-05,
      "loss": 0.5284,
      "step": 2880
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.6122298240661621,
      "learning_rate": 4.5981308411214955e-05,
      "loss": 0.4739,
      "step": 2890
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.6604053378105164,
      "learning_rate": 4.544726301735648e-05,
      "loss": 0.4495,
      "step": 2900
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.5347923040390015,
      "learning_rate": 4.4913217623498e-05,
      "loss": 0.457,
      "step": 2910
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.8845210075378418,
      "learning_rate": 4.437917222963952e-05,
      "loss": 0.5939,
      "step": 2920
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.7520017027854919,
      "learning_rate": 4.384512683578104e-05,
      "loss": 0.4601,
      "step": 2930
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.6583958268165588,
      "learning_rate": 4.331108144192256e-05,
      "loss": 0.4718,
      "step": 2940
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.6407686471939087,
      "learning_rate": 4.277703604806409e-05,
      "loss": 0.518,
      "step": 2950
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.7865018844604492,
      "learning_rate": 4.2242990654205613e-05,
      "loss": 0.4635,
      "step": 2960
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.725208580493927,
      "learning_rate": 4.170894526034713e-05,
      "loss": 0.4863,
      "step": 2970
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.7023617625236511,
      "learning_rate": 4.117489986648865e-05,
      "loss": 0.5056,
      "step": 2980
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.8546898365020752,
      "learning_rate": 4.0640854472630175e-05,
      "loss": 0.5269,
      "step": 2990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.6326480507850647,
      "learning_rate": 4.01068090787717e-05,
      "loss": 0.4745,
      "step": 3000
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.5784589648246765,
      "learning_rate": 3.957276368491322e-05,
      "loss": 0.4841,
      "step": 3010
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.6999981999397278,
      "learning_rate": 3.903871829105474e-05,
      "loss": 0.4433,
      "step": 3020
    },
    {
      "epoch": 2.424,
      "grad_norm": 1.0301588773727417,
      "learning_rate": 3.8504672897196265e-05,
      "loss": 0.4809,
      "step": 3030
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.5390238761901855,
      "learning_rate": 3.797062750333779e-05,
      "loss": 0.4723,
      "step": 3040
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.591255784034729,
      "learning_rate": 3.7436582109479304e-05,
      "loss": 0.4845,
      "step": 3050
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.701238751411438,
      "learning_rate": 3.690253671562083e-05,
      "loss": 0.4853,
      "step": 3060
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.5968871116638184,
      "learning_rate": 3.636849132176235e-05,
      "loss": 0.4561,
      "step": 3070
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.5608807802200317,
      "learning_rate": 3.583444592790388e-05,
      "loss": 0.4616,
      "step": 3080
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.620189905166626,
      "learning_rate": 3.5300400534045395e-05,
      "loss": 0.4774,
      "step": 3090
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.8136149048805237,
      "learning_rate": 3.476635514018692e-05,
      "loss": 0.4938,
      "step": 3100
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.5970533490180969,
      "learning_rate": 3.423230974632844e-05,
      "loss": 0.5356,
      "step": 3110
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.5116239786148071,
      "learning_rate": 3.369826435246996e-05,
      "loss": 0.4561,
      "step": 3120
    },
    {
      "epoch": 2.504,
      "grad_norm": 2.7803268432617188,
      "learning_rate": 3.316421895861148e-05,
      "loss": 0.4957,
      "step": 3130
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.7225194573402405,
      "learning_rate": 3.263017356475301e-05,
      "loss": 0.4881,
      "step": 3140
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.6218999624252319,
      "learning_rate": 3.209612817089453e-05,
      "loss": 0.482,
      "step": 3150
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.7813843488693237,
      "learning_rate": 3.156208277703605e-05,
      "loss": 0.454,
      "step": 3160
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.898488461971283,
      "learning_rate": 3.102803738317757e-05,
      "loss": 0.4823,
      "step": 3170
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.7170093655586243,
      "learning_rate": 3.0493991989319092e-05,
      "loss": 0.4769,
      "step": 3180
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.6480295062065125,
      "learning_rate": 2.9959946595460615e-05,
      "loss": 0.4789,
      "step": 3190
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.8086351752281189,
      "learning_rate": 2.9425901201602134e-05,
      "loss": 0.5433,
      "step": 3200
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.7345523238182068,
      "learning_rate": 2.8891855807743663e-05,
      "loss": 0.4941,
      "step": 3210
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.9566168189048767,
      "learning_rate": 2.8357810413885183e-05,
      "loss": 0.4843,
      "step": 3220
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.6495538353919983,
      "learning_rate": 2.7823765020026705e-05,
      "loss": 0.4592,
      "step": 3230
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.6051826477050781,
      "learning_rate": 2.7289719626168225e-05,
      "loss": 0.4632,
      "step": 3240
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.8690407276153564,
      "learning_rate": 2.6755674232309747e-05,
      "loss": 0.4309,
      "step": 3250
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.8331804275512695,
      "learning_rate": 2.6221628838451267e-05,
      "loss": 0.4618,
      "step": 3260
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.659271776676178,
      "learning_rate": 2.5687583444592793e-05,
      "loss": 0.4782,
      "step": 3270
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.5854448080062866,
      "learning_rate": 2.5153538050734315e-05,
      "loss": 0.4878,
      "step": 3280
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.7458723783493042,
      "learning_rate": 2.4619492656875838e-05,
      "loss": 0.4773,
      "step": 3290
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.656330943107605,
      "learning_rate": 2.4085447263017357e-05,
      "loss": 0.485,
      "step": 3300
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.6625690460205078,
      "learning_rate": 2.355140186915888e-05,
      "loss": 0.4648,
      "step": 3310
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.5772973895072937,
      "learning_rate": 2.3017356475300403e-05,
      "loss": 0.4457,
      "step": 3320
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.6736907362937927,
      "learning_rate": 2.2483311081441925e-05,
      "loss": 0.4826,
      "step": 3330
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.6089690923690796,
      "learning_rate": 2.1949265687583445e-05,
      "loss": 0.4758,
      "step": 3340
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.6552948355674744,
      "learning_rate": 2.1415220293724967e-05,
      "loss": 0.4808,
      "step": 3350
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.5775200128555298,
      "learning_rate": 2.088117489986649e-05,
      "loss": 0.4646,
      "step": 3360
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.7867649793624878,
      "learning_rate": 2.0347129506008013e-05,
      "loss": 0.4829,
      "step": 3370
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.7444033622741699,
      "learning_rate": 1.9813084112149532e-05,
      "loss": 0.4501,
      "step": 3380
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.7611954212188721,
      "learning_rate": 1.9279038718291058e-05,
      "loss": 0.4616,
      "step": 3390
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.7622140645980835,
      "learning_rate": 1.8744993324432577e-05,
      "loss": 0.4602,
      "step": 3400
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.9363883137702942,
      "learning_rate": 1.82109479305741e-05,
      "loss": 0.4723,
      "step": 3410
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.8659857511520386,
      "learning_rate": 1.7676902536715623e-05,
      "loss": 0.4853,
      "step": 3420
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.7670533061027527,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 0.4,
      "step": 3430
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.6466801762580872,
      "learning_rate": 1.6608811748998665e-05,
      "loss": 0.493,
      "step": 3440
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.6589447855949402,
      "learning_rate": 1.607476635514019e-05,
      "loss": 0.4379,
      "step": 3450
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.7788187861442566,
      "learning_rate": 1.554072096128171e-05,
      "loss": 0.4731,
      "step": 3460
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.7675331830978394,
      "learning_rate": 1.500667556742323e-05,
      "loss": 0.447,
      "step": 3470
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.90761798620224,
      "learning_rate": 1.4472630173564752e-05,
      "loss": 0.495,
      "step": 3480
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.797194242477417,
      "learning_rate": 1.3938584779706276e-05,
      "loss": 0.4828,
      "step": 3490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6243364214897156,
      "learning_rate": 1.3404539385847797e-05,
      "loss": 0.4503,
      "step": 3500
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.761124849319458,
      "learning_rate": 1.2870493991989318e-05,
      "loss": 0.5091,
      "step": 3510
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.673845112323761,
      "learning_rate": 1.233644859813084e-05,
      "loss": 0.451,
      "step": 3520
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.5362681150436401,
      "learning_rate": 1.1802403204272363e-05,
      "loss": 0.4513,
      "step": 3530
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.7024778723716736,
      "learning_rate": 1.1268357810413886e-05,
      "loss": 0.4553,
      "step": 3540
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.8088148832321167,
      "learning_rate": 1.0734312416555407e-05,
      "loss": 0.4891,
      "step": 3550
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.7170732021331787,
      "learning_rate": 1.020026702269693e-05,
      "loss": 0.498,
      "step": 3560
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.7238800525665283,
      "learning_rate": 9.66622162883845e-06,
      "loss": 0.4535,
      "step": 3570
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.7201805114746094,
      "learning_rate": 9.132176234979973e-06,
      "loss": 0.4472,
      "step": 3580
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.8109341859817505,
      "learning_rate": 8.598130841121496e-06,
      "loss": 0.4788,
      "step": 3590
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.7580430507659912,
      "learning_rate": 8.064085447263017e-06,
      "loss": 0.4789,
      "step": 3600
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.6884029507637024,
      "learning_rate": 7.53004005340454e-06,
      "loss": 0.4309,
      "step": 3610
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.7363259196281433,
      "learning_rate": 6.9959946595460625e-06,
      "loss": 0.445,
      "step": 3620
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.9371945858001709,
      "learning_rate": 6.4619492656875834e-06,
      "loss": 0.488,
      "step": 3630
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.6956119537353516,
      "learning_rate": 5.927903871829106e-06,
      "loss": 0.472,
      "step": 3640
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.6901775598526001,
      "learning_rate": 5.393858477970628e-06,
      "loss": 0.4719,
      "step": 3650
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.6247724890708923,
      "learning_rate": 4.85981308411215e-06,
      "loss": 0.5194,
      "step": 3660
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.7439277768135071,
      "learning_rate": 4.325767690253672e-06,
      "loss": 0.4611,
      "step": 3670
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.6611643433570862,
      "learning_rate": 3.7917222963951934e-06,
      "loss": 0.4774,
      "step": 3680
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.6179123520851135,
      "learning_rate": 3.257676902536716e-06,
      "loss": 0.4912,
      "step": 3690
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.753463625907898,
      "learning_rate": 2.723631508678238e-06,
      "loss": 0.4908,
      "step": 3700
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.5648813843727112,
      "learning_rate": 2.1895861148197598e-06,
      "loss": 0.441,
      "step": 3710
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.5826159119606018,
      "learning_rate": 1.6555407209612818e-06,
      "loss": 0.4501,
      "step": 3720
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.7140342593193054,
      "learning_rate": 1.1214953271028036e-06,
      "loss": 0.484,
      "step": 3730
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.7748411297798157,
      "learning_rate": 5.874499332443259e-07,
      "loss": 0.4751,
      "step": 3740
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.6940228939056396,
      "learning_rate": 5.34045393858478e-08,
      "loss": 0.4631,
      "step": 3750
    }
  ],
  "logging_steps": 10,
  "max_steps": 3750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.846184354481439e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
