wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

================================================================================
Training Configuration: config4_r16_lr1e4
GPU: 3 | LoRA r: 16 | LR: 0.0001
Batch size: 2 | Grad accum: 4
Max seq length: 1024 | Samples: 10000
================================================================================

Loading model and tokenizer...
==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 4. Max memory: 15.477 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading dataset...
Training samples: 10000
Validation samples: 500
Formatting training dataset...
Map: 100%|██████████| 10000/10000 [00:00<00:00, 35337.18 examples/s]
Setting up LoRA...
Unsloth 2025.10.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Configuring trainer...
Unsloth: Tokenizing ["text"] (num_proc=100): 100%|██████████| 10000/10000 [00:23<00:00, 432.87 examples/s]

================================================================================
Starting training: config4_r16_lr1e4
================================================================================
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 10,000 | Num Epochs = 3 | Total steps = 939
O^O/ \_/ \    Batch size per device = 8 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32
 "-____-"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)
  3%|▎         | 24/939 [07:04<4:19:09, 16.99s/it]

Unsloth: Will smartly offload gradients to save VRAM!
