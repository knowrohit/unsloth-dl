wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

================================================================================
Training Configuration: config4_r16_lr1e4
GPU: 3 | LoRA r: 16 | LR: 0.0001
Batch size: 2 | Grad accum: 4
Max seq length: 1024 | Samples: 10000
================================================================================

Loading model and tokenizer...
==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.477 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading dataset...
Training samples: 10000
Validation samples: 500
Formatting training dataset...
Setting up LoRA...
Unsloth 2025.10.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Configuring trainer...

================================================================================
Starting training: config4_r16_lr1e4
================================================================================
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 10,000 | Num Epochs = 3 | Total steps = 3,750
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)
                                                       

Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.8778, 'grad_norm': 0.42847388982772827, 'learning_rate': 9.882510013351135e-05, 'epoch': 0.04}
{'loss': 0.709, 'grad_norm': 0.4465162754058838, 'learning_rate': 9.748998664886516e-05, 'epoch': 0.08}
{'loss': 0.7029, 'grad_norm': 0.41149336099624634, 'learning_rate': 9.615487316421896e-05, 'epoch': 0.12}
{'loss': 0.7269, 'grad_norm': 0.40224599838256836, 'learning_rate': 9.481975967957278e-05, 'epoch': 0.16}
{'loss': 0.7062, 'grad_norm': 0.4298059046268463, 'learning_rate': 9.348464619492658e-05, 'epoch': 0.2}
{'loss': 0.6889, 'grad_norm': 0.377262681722641, 'learning_rate': 9.214953271028038e-05, 'epoch': 0.24}
{'loss': 0.6881, 'grad_norm': 0.3450877070426941, 'learning_rate': 9.081441922563419e-05, 'epoch': 0.28}
{'loss': 0.6745, 'grad_norm': 0.4000623822212219, 'learning_rate': 8.947930574098799e-05, 'epoch': 0.32}
{'loss': 0.6816, 'grad_norm': 0.4251692295074463, 'learning_rate': 8.814419225634179e-05, 'epoch': 0.36}
{'loss': 0.6658, 'grad_norm': 0.35269439220428467, 'learning_rate': 8.680907877169559e-05, 'epoch': 0.4}
{'loss': 0.683, 'grad_norm': 0.3366101086139679, 'learning_rate': 8.54739652870494e-05, 'epoch': 0.44}
{'loss': 0.6703, 'grad_norm': 0.5217921733856201, 'learning_rate': 8.41388518024032e-05, 'epoch': 0.48}
{'loss': 0.6771, 'grad_norm': 0.36145704984664917, 'learning_rate': 8.280373831775702e-05, 'epoch': 0.52}
{'loss': 0.659, 'grad_norm': 0.344753623008728, 'learning_rate': 8.146862483311082e-05, 'epoch': 0.56}
{'loss': 0.6691, 'grad_norm': 0.39573541283607483, 'learning_rate': 8.013351134846462e-05, 'epoch': 0.6}
{'loss': 0.6596, 'grad_norm': 0.45842084288597107, 'learning_rate': 7.879839786381843e-05, 'epoch': 0.64}
{'loss': 0.6697, 'grad_norm': 0.3821866512298584, 'learning_rate': 7.746328437917223e-05, 'epoch': 0.68}
{'loss': 0.6701, 'grad_norm': 0.4388541877269745, 'learning_rate': 7.612817089452604e-05, 'epoch': 0.72}
{'loss': 0.6645, 'grad_norm': 0.553843080997467, 'learning_rate': 7.479305740987984e-05, 'epoch': 0.76}
{'loss': 0.6499, 'grad_norm': 0.4527105987071991, 'learning_rate': 7.345794392523366e-05, 'epoch': 0.8}
{'loss': 0.6614, 'grad_norm': 0.49427148699760437, 'learning_rate': 7.212283044058746e-05, 'epoch': 0.84}
{'loss': 0.6633, 'grad_norm': 0.4178614914417267, 'learning_rate': 7.078771695594127e-05, 'epoch': 0.88}
{'loss': 0.6436, 'grad_norm': 0.3969365358352661, 'learning_rate': 6.945260347129507e-05, 'epoch': 0.92}
{'loss': 0.6662, 'grad_norm': 0.43888285756111145, 'learning_rate': 6.811748998664887e-05, 'epoch': 0.96}
{'loss': 0.6519, 'grad_norm': 0.44097816944122314, 'learning_rate': 6.678237650200267e-05, 'epoch': 1.0}
{'loss': 0.558, 'grad_norm': 0.48172569274902344, 'learning_rate': 6.544726301735647e-05, 'epoch': 1.04}
{'loss': 0.5745, 'grad_norm': 0.5555459856987, 'learning_rate': 6.411214953271028e-05, 'epoch': 1.08}
{'loss': 0.5736, 'grad_norm': 0.5163300037384033, 'learning_rate': 6.277703604806408e-05, 'epoch': 1.12}
{'loss': 0.577, 'grad_norm': 0.5806172490119934, 'learning_rate': 6.14419225634179e-05, 'epoch': 1.16}
{'loss': 0.5537, 'grad_norm': 0.6470587849617004, 'learning_rate': 6.0106809078771696e-05, 'epoch': 1.2}
{'loss': 0.5744, 'grad_norm': 0.5027607083320618, 'learning_rate': 5.877169559412551e-05, 'epoch': 1.24}
{'loss': 0.5593, 'grad_norm': 0.5191686749458313, 'learning_rate': 5.743658210947931e-05, 'epoch': 1.28}
{'loss': 0.5854, 'grad_norm': 0.5725610852241516, 'learning_rate': 5.610146862483311e-05, 'epoch': 1.32}
{'loss': 0.5511, 'grad_norm': 0.5446178317070007, 'learning_rate': 5.476635514018692e-05, 'epoch': 1.36}
{'loss': 0.5602, 'grad_norm': 0.5289411544799805, 'learning_rate': 5.343124165554072e-05, 'epoch': 1.4}
{'loss': 0.5708, 'grad_norm': 0.49646350741386414, 'learning_rate': 5.209612817089453e-05, 'epoch': 1.44}
{'loss': 0.5558, 'grad_norm': 0.6082926392555237, 'learning_rate': 5.076101468624833e-05, 'epoch': 1.48}
{'loss': 0.5538, 'grad_norm': 0.5215615630149841, 'learning_rate': 4.9425901201602136e-05, 'epoch': 1.52}
{'loss': 0.5446, 'grad_norm': 0.6236454844474792, 'learning_rate': 4.809078771695594e-05, 'epoch': 1.56}
{'loss': 0.5501, 'grad_norm': 0.6370816230773926, 'learning_rate': 4.675567423230975e-05, 'epoch': 1.6}
{'loss': 0.5493, 'grad_norm': 0.7290992140769958, 'learning_rate': 4.5420560747663556e-05, 'epoch': 1.64}
{'loss': 0.5274, 'grad_norm': 0.6814956665039062, 'learning_rate': 4.408544726301736e-05, 'epoch': 1.68}
{'loss': 0.5509, 'grad_norm': 0.7019141912460327, 'learning_rate': 4.275033377837116e-05, 'epoch': 1.72}
{'loss': 0.5389, 'grad_norm': 0.6828774213790894, 'learning_rate': 4.141522029372497e-05, 'epoch': 1.76}
{'loss': 0.5299, 'grad_norm': 0.7184171676635742, 'learning_rate': 4.008010680907877e-05, 'epoch': 1.8}
{'loss': 0.5501, 'grad_norm': 0.7121848464012146, 'learning_rate': 3.8744993324432575e-05, 'epoch': 1.84}
{'loss': 0.5451, 'grad_norm': 0.7249907851219177, 'learning_rate': 3.740987983978638e-05, 'epoch': 1.88}
{'loss': 0.5576, 'grad_norm': 0.635041356086731, 'learning_rate': 3.607476635514019e-05, 'epoch': 1.92}
{'loss': 0.5568, 'grad_norm': 0.660753607749939, 'learning_rate': 3.4739652870493996e-05, 'epoch': 1.96}
{'loss': 0.5284, 'grad_norm': 0.5418767333030701, 'learning_rate': 3.34045393858478e-05, 'epoch': 2.0}
{'loss': 0.4396, 'grad_norm': 0.6570726037025452, 'learning_rate': 3.20694259012016e-05, 'epoch': 2.04}
{'loss': 0.4324, 'grad_norm': 0.8614078164100647, 'learning_rate': 3.073431241655541e-05, 'epoch': 2.08}
{'loss': 0.4438, 'grad_norm': 0.6731128692626953, 'learning_rate': 2.9399198931909212e-05, 'epoch': 2.12}
{'loss': 0.4444, 'grad_norm': 0.8526785373687744, 'learning_rate': 2.8064085447263015e-05, 'epoch': 2.16}
{'loss': 0.4391, 'grad_norm': 1.034263253211975, 'learning_rate': 2.6728971962616822e-05, 'epoch': 2.2}
{'loss': 0.434, 'grad_norm': 0.9058825373649597, 'learning_rate': 2.539385847797063e-05, 'epoch': 2.24}
{'loss': 0.4413, 'grad_norm': 0.6560150980949402, 'learning_rate': 2.4058744993324432e-05, 'epoch': 2.28}
{'loss': 0.432, 'grad_norm': 0.7785441875457764, 'learning_rate': 2.272363150867824e-05, 'epoch': 2.32}
{'loss': 0.462, 'grad_norm': 0.7597134709358215, 'learning_rate': 2.1388518024032045e-05, 'epoch': 2.36}
{'loss': 0.4534, 'grad_norm': 0.6788946986198425, 'learning_rate': 2.005340453938585e-05, 'epoch': 2.4}
{'loss': 0.4366, 'grad_norm': 0.613195538520813, 'learning_rate': 1.8718291054739652e-05, 'epoch': 2.44}
{'loss': 0.4355, 'grad_norm': 0.9719125628471375, 'learning_rate': 1.738317757009346e-05, 'epoch': 2.48}
{'loss': 0.446, 'grad_norm': 1.0229040384292603, 'learning_rate': 1.6048064085447265e-05, 'epoch': 2.52}
{'loss': 0.4483, 'grad_norm': 1.0333019495010376, 'learning_rate': 1.4712950600801067e-05, 'epoch': 2.56}
{'loss': 0.4266, 'grad_norm': 0.8833162188529968, 'learning_rate': 1.3377837116154874e-05, 'epoch': 2.6}
{'loss': 0.4416, 'grad_norm': 0.7412859797477722, 'learning_rate': 1.2042723631508679e-05, 'epoch': 2.64}
{'loss': 0.4327, 'grad_norm': 0.7831760048866272, 'learning_rate': 1.0707610146862484e-05, 'epoch': 2.68}
{'loss': 0.4256, 'grad_norm': 0.9380245208740234, 'learning_rate': 9.372496662216289e-06, 'epoch': 2.72}
{'loss': 0.4196, 'grad_norm': 0.8738373517990112, 'learning_rate': 8.037383177570095e-06, 'epoch': 2.76}
{'loss': 0.4294, 'grad_norm': 0.7412367463111877, 'learning_rate': 6.702269692923899e-06, 'epoch': 2.8}
{'loss': 0.4354, 'grad_norm': 0.9855321645736694, 'learning_rate': 5.3671562082777036e-06, 'epoch': 2.84}
{'loss': 0.433, 'grad_norm': 0.9198365807533264, 'learning_rate': 4.0320427236315086e-06, 'epoch': 2.88}
{'loss': 0.4242, 'grad_norm': 0.7200182676315308, 'learning_rate': 2.696929238985314e-06, 'epoch': 2.92}
{'loss': 0.4482, 'grad_norm': 0.9494790434837341, 'learning_rate': 1.361815754339119e-06, 'epoch': 2.96}
{'loss': 0.4252, 'grad_norm': 0.7408268451690674, 'learning_rate': 2.67022696929239e-08, 'epoch': 3.0}
{'train_runtime': 15200.2742, 'train_samples_per_second': 1.974, 'train_steps_per_second': 0.247, 'train_loss': 0.5584921310424805, 'epoch': 3.0}

================================================================================
Training completed: config4_r16_lr1e4
================================================================================

Saving model...
Done! Model saved to ./models/config4_r16_lr1e4
