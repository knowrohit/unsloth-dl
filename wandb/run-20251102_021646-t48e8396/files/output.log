wandb: Detected [huggingface_hub.inference] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

================================================================================
Training Configuration: config5_r1_alpha2_lr2e4_maxsteps60
GPU: 0 | LoRA r: 1 | LoRA alpha: 2 | LR: 0.0002
Batch size: 2 | Grad accum: 4
Max seq length: 1024 | Samples: 10000
Training duration: 60 steps
================================================================================

Loading model and tokenizer...
==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 5060 Ti. Num GPUs = 1. Max memory: 15.477 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading dataset...
Training samples: 10000
Validation samples: 500
Formatting training dataset...
Setting up LoRA...
Unsloth 2025.10.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Configuring trainer...

================================================================================
Starting training: config5_r1_alpha2_lr2e4_maxsteps60
================================================================================
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 10,000 | Num Epochs = 1 | Total steps = 60
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 2,621,440 of 8,032,882,688 (0.03% trained)
100%|██████████| 60/60 [04:01<00:00,  4.02s/it]

Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.4839, 'grad_norm': 0.5885986685752869, 'learning_rate': 0.00018545454545454545, 'epoch': 0.01}
{'loss': 1.1068, 'grad_norm': 0.5683570504188538, 'learning_rate': 0.0001490909090909091, 'epoch': 0.02}
{'loss': 0.8263, 'grad_norm': 0.42210444808006287, 'learning_rate': 0.00011272727272727272, 'epoch': 0.02}
{'loss': 0.8369, 'grad_norm': 0.40950921177864075, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.03}
{'loss': 0.7852, 'grad_norm': 0.39615148305892944, 'learning_rate': 4e-05, 'epoch': 0.04}
{'loss': 0.7949, 'grad_norm': 0.4651467502117157, 'learning_rate': 3.636363636363636e-06, 'epoch': 0.05}
{'train_runtime': 241.2174, 'train_samples_per_second': 1.99, 'train_steps_per_second': 0.249, 'train_loss': 0.9723345041275024, 'epoch': 0.05}

================================================================================
Training completed: config5_r1_alpha2_lr2e4_maxsteps60
================================================================================

Saving model...
Done! Model saved to ./models/config5_r1_alpha2_lr2e4_maxsteps60
